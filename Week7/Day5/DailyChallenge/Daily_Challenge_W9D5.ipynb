{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3689760c-41f8-4a33-9c96-3fd17803950e",
        "_uuid": "3e0ad409d438c7c68ea6a76700a1e964a357453f",
        "id": "pPpG7qvYnaJZ"
      },
      "source": [
        "<h1 align=\"center\"> Credit Fraud Detector </h1>\n",
        "\n",
        "<h2> Introduction </h2>\n",
        "In this kernel we will use various predictive models to see how accurate they  are in detecting whether a transaction is a normal payment or a fraud. As described in the dataset, the features are scaled and the names of the features are not shown due to privacy reasons. Nevertheless, we can still analyze some important aspects of the dataset. Let's start!\n",
        "\n",
        "\n",
        "<h2> Our Goals: </h2>\n",
        "<ul>\n",
        "<li> Understand the little distribution of the \"little\" data that was provided to us. </li>\n",
        "<li> Create a 50/50 sub-dataframe ratio of \"Fraud\" and \"Non-Fraud\" transactions. (NearMiss Algorithm) </li>\n",
        "\n",
        "\n",
        "\n",
        "<h2> Outline: </h2>\n",
        "I. <b>Understanding our data</b><br>\n",
        "a) [Gather Sense of our data](#gather)<br><br>\n",
        "\n",
        "II. <b>Preprocessing</b><br>\n",
        "a) [Scaling and Distributing](#distributing)<br>\n",
        "b) [Splitting the Data](#splitting)<br><br>\n",
        "\n",
        "III. <b>Random UnderSampling and Oversampling</b><br>\n",
        "a) [Distributing and Correlating](#correlating)<br>\n",
        "b) [Anomaly Detection](#anomaly)<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ae8dd7f3-80a7-4db9-a132-823b0e48c041",
        "_uuid": "c999e5f1ac81513263d83883008f2844209e9e07",
        "id": "qUDBtf1hnaJa"
      },
      "source": [
        "## Gather Sense of Our Data:\n",
        "<a id=\"gather\"></a>\n",
        "The first thing we must do is gather a <b> basic sense </b> of our data. Remember, except for the <b>transaction</b> and <b>amount</b> we dont know what the other columns are (due to privacy reasons). The only thing we know, is that those columns that are unknown have been scaled already.   \n",
        "\n",
        "<h3> Summary: </h3>\n",
        "<ul>\n",
        "<li>The transaction amount is relatively <b>small</b>. The mean of all the mounts made is approximately USD 88. </li>\n",
        "<li>There are no <b>\"Null\"</b> values, so we don't have to work on ways to replace values. </li>\n",
        "<li> Most of the transactions were <b>Non-Fraud</b> (99.83%) of the time, while <b>Fraud</b> transactions occurs (017%) of the time in the dataframe. </li>\n",
        "</ul>\n",
        "\n",
        "<h3> Feature Technicalities: </h3>\n",
        "<ul>\n",
        "<li> <b>PCA Transformation: </b>  The description of the data says that all the features went through a PCA transformation (Dimensionality Reduction technique) (Except for time and amount).</li>\n",
        "<li> <b>Scaling:</b> Keep in mind that in order to implement a PCA transformation features need to be previously scaled. (In this case, all the V features have been scaled or at least that is what we are assuming the people that develop the dataset did.)</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "9exxW6PXnaJb",
        "trusted": true
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m \u001b[38;5;66;03m# linear algebra\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m \u001b[38;5;66;03m# data processing, CSV file I/O (e.g. pd.read_csv)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
          ]
        }
      ],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in\n",
        "\n",
        "# Imported Libraries\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "import matplotlib.patches as mpatches\n",
        "import time\n",
        "\n",
        "# Classifier Libraries\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import collections\n",
        "\n",
        "\n",
        "# Other Libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import NearMiss\n",
        "from imblearn.metrics import classification_report_imbalanced\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSjcr26JpQ4j"
      },
      "outputs": [],
      "source": [
        "# TODO : import the dataset available here : https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "376ce881-463a-4a09-9ac0-c63f85577eec",
        "_kg_hide-input": true,
        "_uuid": "93031e732e5aca3a2b4984799d6bf58d76e4b52d",
        "id": "IzK0mg6qnaJd",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# TODO : describe the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "03ddb929-5bc8-4af4-90cd-21dcbb57560d",
        "_kg_hide-input": true,
        "_uuid": "38bec67888aa534e9739e95ef9fac62d27a87021",
        "id": "WHXG7NyRnaJf",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# TODO : Is there any missing values?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6a526b6c-8463-4f6f-92b0-e8a3a21cbb2e",
        "_kg_hide-input": true,
        "_uuid": "479a5f12d3dd68262316a17b4b7b3499e0a2cbe0",
        "id": "Dr6dhNnZnaJf",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# TODO : Are the classes skewed ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "01c007fa-0fcc-4eea-84ff-0861a2f8c533",
        "_kg_hide-input": true,
        "_uuid": "f6b96ff34855e3bf7af1f6979342b01c473e4e07",
        "id": "Lcq4S5F-naJg",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "print('No Frauds', round(df['Class'].value_counts()[0]/len(df) * 100,2), '% of the dataset')\n",
        "print('Frauds', round(df['Class'].value_counts()[1]/len(df) * 100,2), '% of the dataset')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "558c9b60-3f52-4da5-92fa-9fc4acbdbb3a",
        "_uuid": "c2bb0945a312508e908386fc87adc227f0afe0e0",
        "id": "Rsslkj5dnaJg"
      },
      "source": [
        "**Note:**  Notice how imbalanced is our original dataset! Most of the transactions are non-fraud. If we use this dataframe as the base for our predictive models and analysis we might get a lot of errors and our algorithms will probably overfit since it will \"assume\" that most transactions are not fraud. But we don't want our model to assume, we want our model to detect patterns that give signs of fraud!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "657bc987-4b15-4cfa-b290-c39a2632e2ac",
        "_kg_hide-input": true,
        "_uuid": "337caaf6ed3f65beedb24a74eebb22d97ff52ba4",
        "id": "NvJ1H4VLnaJg",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#TODO : Create a count plot using Seaborn to visualize the distribution of classes in the dataframe df, indicating non-fraudulent (0) and fraudulent (1) transactions, with specific colors for each class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3c9973d0-83bd-4b09-860e-c1f507f88310",
        "_uuid": "6894af2afdbfd5cd670d00b66f10ae49f1cab421",
        "id": "zNveiwlWnaJh"
      },
      "source": [
        "**Distributions:** By seeing the distributions we can have an idea how skewed are these features, we can also see further distributions of the other features. There are techniques that can help the distributions be less skewed which will be implemented in this notebook in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cee315f2-325f-42b6-a640-736f10c272cc",
        "_kg_hide-input": true,
        "_uuid": "cfa51792bf6f8a6b318ae1bffcff4e922b1d1917",
        "id": "zM9izhy4naJh",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# TODO : Generate two distribution plots side by side, one for transaction amounts and the other for transaction times, using the data from the df dataframe, with specific colors and custom title and x-axis limits for each plot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "72fdda5e-7f82-488d-a433-6157d6180bb8",
        "_uuid": "c5d6781e61c0ee84e72d26e8465bfd98ef91f3b9",
        "id": "fsKHD3RwnaJh"
      },
      "source": [
        "<h2> Scaling and Distributing </h2>\n",
        "<a id=\"distributing\"></a>\n",
        "In this phase of our kernel, we will first scale the columns comprise of <b>Time</b> and <b>Amount </b>. Time and amount should be scaled as the other columns. On the other hand, we need to also create a sub sample of the dataframe in order to have an equal amount of Fraud and Non-Fraud cases, helping our algorithms better understand patterns that determines whether a transaction is a fraud or not.\n",
        "\n",
        "<h3> What is a sub-Sample?</h3>\n",
        "In this scenario, our subsample will be a dataframe with a 50/50 ratio of fraud and non-fraud transactions. Meaning our sub-sample will have the same amount of fraud and non fraud transactions.\n",
        "\n",
        "<h3> Why do we create a sub-Sample?</h3>\n",
        "In the beginning of this notebook we saw that the original dataframe was heavily imbalanced! Using the original dataframe  will cause the following issues:\n",
        "<ul>\n",
        "<li><b>Overfitting: </b>Our classification models will assume that in most cases there are no frauds! What we want for our model is to be certain when a fraud occurs. </li>\n",
        "<li><b>Wrong Correlations:</b> Although we don't know what the \"V\" features stand for, it will be useful to understand how each of this features influence the result (Fraud or No Fraud) by having an imbalance dataframe we are not able to see the true correlations between the class and features. </li>\n",
        "</ul>\n",
        "\n",
        "<h3>Summary: </h3>\n",
        "<ul>\n",
        "<li> <b>Scaled amount </b> and <b> scaled time </b> are the columns with scaled values. </li>\n",
        "<li> There are <b>492 cases </b> of fraud in our dataset so we can randomly get 492 cases of non-fraud to create our new sub dataframe. </li>\n",
        "<li>We concat the 492 cases of fraud and non fraud, <b>creating a new sub-sample. </b></li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d5d64bf0-2fbb-4096-a265-f68887bf2fde",
        "_kg_hide-input": true,
        "_uuid": "1501ec379c9b5c39c3857ba0febd0aedee9c30d5",
        "id": "LODknUUdnaJh",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# TODO : Scale the 'Amount' and 'Time' columns of the dataframe df using RobustScaler to mitigate the influence of outliers.\n",
        "#TODO : Replace the original columns with the scaled versions.\n",
        "#TODO : Remove the original 'Time' and 'Amount' columns from the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cdb9bb1e-9fab-4fd1-a409-468ba8bc36ee",
        "_kg_hide-input": true,
        "_uuid": "a33d701247ab45d849c5e94735346a738a6c6970",
        "id": "wsPzLlpZnaJi",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#TODO : Remove the 'scaled_amount' and 'scaled_time' columns from the dataframe df.\n",
        "#TODO : Reinserts these columns at the beginning, ensuring 'scaled_amount' and 'scaled_time' are the first two columns in the dataframe, effectively reordering the columns to prioritize the scaled versions of amount and time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a59c8c8d-a4bc-4671-aa2f-9f959c142cde",
        "_uuid": "5119c4ea9e0b9031dbc5937b56323da224985024",
        "id": "YTOd6XwwnaJi"
      },
      "source": [
        "### Splitting the Data (Original DataFrame)\n",
        "<a id=\"splitting\"></a>\n",
        "Before proceeding with the <b> Random UnderSampling technique</b> we have to separate the orginal dataframe. <b> Why? for testing purposes, remember although we are splitting the data when implementing Random UnderSampling or OverSampling techniques, we want to test our models on the original testing set not on the testing set created by either of these techniques.</b> The main goal is to fit the model either with the dataframes that were undersample and oversample (in order for our models to detect the patterns), and test it on the original testing set.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c6c962cc-6f38-4a00-bcd7-ce9d91db954c",
        "_kg_hide-input": true,
        "_uuid": "9f7b5d920703b3a3c8c0f62bc6042e4615bc8324",
        "id": "TgjJh5HRnaJj",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#TODO : calculate and prints the percentage of non-fraudulent and fraudulent transactions in the dataset.\n",
        "#TODO : prepare the data for model training by splitting the dataset into features (X) and the target variable (y).\n",
        "#TODO : utilize StratifiedKFold from scikit-learn for cross-validation, ensuring that each fold of the dataset has the same proportion of fraudulent and non-fraudulent transactions as the original dataset\n",
        "#TODO : Iterate through these splits to print the train and test indices, also creating training and testing datasets for both features and target variable.\n",
        "\n",
        "# We already have X_train and y_train for undersample data thats why I am using original to distinguish and to not overwrite these variables.\n",
        "# original_Xtrain, original_Xtest, original_ytrain, original_ytest = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check the Distribution of the labels\n",
        "\n",
        "\n",
        "# Turn into an array\n",
        "original_Xtrain = original_Xtrain.values\n",
        "original_Xtest = original_Xtest.values\n",
        "original_ytrain = original_ytrain.values\n",
        "original_ytest = original_ytest.values\n",
        "\n",
        "# TODO : See if both the train and test label distribution are similarly distributed\n",
        "\n",
        "\n",
        "print('Label Distributions: \\n')\n",
        "print(train_counts_label/ len(original_ytrain))\n",
        "print(test_counts_label/ len(original_ytest))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "956d34b9-e562-4b70-a2f8-fbe060273a83",
        "_uuid": "cc554c4ffec656cb38d01c034f2cd338e1cb4565",
        "id": "VCIMteGXnaJk"
      },
      "source": [
        "## Random Under-Sampling:\n",
        "<img src=\"http://contrib.scikit-learn.org/imbalanced-learn/stable/_images/sphx_glr_plot_random_under_sampler_001.png\">\n",
        "\n",
        "In this phase of the project we will implement *\"Random Under Sampling\"* which basically consists of removing data in order to have a more <b> balanced dataset </b> and thus avoiding our models to overfitting.\n",
        "\n",
        "#### Steps:\n",
        "<ul>\n",
        "<li>The first thing we have to do is determine how <b>imbalanced</b> is our class (use \"value_counts()\" on the class column to determine the amount for each label)  </li>\n",
        "<li>Once we determine how many instances are considered <b>fraud transactions </b> (Fraud = \"1\") , we should bring the <b>non-fraud transactions</b> to the same amount as fraud transactions (assuming we want a 50/50 ratio), this will be equivalent to 492 cases of fraud and 492 cases of non-fraud transactions.  </li>\n",
        "<li> After implementing this technique, we have a sub-sample of our dataframe with a 50/50 ratio with regards to our classes. Then the next step we will implement is to <b>shuffle the data</b> to see if our models can maintain a certain accuracy everytime we run this script.</li>\n",
        "</ul>\n",
        "\n",
        "**Note:** The main issue with \"Random Under-Sampling\" is that we run the risk that our classification models will not perform as accurate as we would like to since there is a great deal of <b>information loss</b> (bringing 492 non-fraud transaction  from 284,315 non-fraud transaction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f0acfc44-eb2a-4356-ad03-d0c12807acd7",
        "_kg_hide-input": true,
        "_uuid": "e3a2b89752681164f14c8273452fc66734d7f41b",
        "id": "ovLQcx_3naJk",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Since our classes are highly skewed we should make them equivalent in order to have a normal distribution of the classes.\n",
        "\n",
        "# Lets shuffle the data before creating the subsamples\n",
        "\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "# amount of fraud classes 492 rows.\n",
        "fraud_df = df.loc[df['Class'] == 1]\n",
        "non_fraud_df = df.loc[df['Class'] == 0][:492]\n",
        "\n",
        "normal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n",
        "\n",
        "# Shuffle dataframe rows\n",
        "new_df = normal_distributed_df.sample(frac=1, random_state=42)\n",
        "\n",
        "new_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "77198464-c0f8-4694-ac0b-4b29b94d0da3",
        "_uuid": "b6818122806657e7accb8be1f4bf17086bb9b149",
        "id": "u3YPtvOKnaJk"
      },
      "source": [
        "##  Equally Distributing and Correlating:\n",
        "<a id=\"correlating\"></a>\n",
        "Now that we have our dataframe correctly balanced, we can go further with our <b>analysis</b> and <b>data preprocessing</b>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "73454100-dc69-49fd-b1b2-f72e326bca5d",
        "_kg_hide-input": true,
        "_uuid": "68b42e92df59f10fbd3ba700389796c4506af604",
        "id": "SXQVJVb0naJk",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#TODO : print the distribution of the classes (presumably fraud and non-fraud transactions) in the subsample dataset new_df as proportions.\n",
        "#TODO : use seaborn's countplot function to visualize the distribution of the classes in new_df.\n",
        "#TODO : apply a predefined color palette colors to differentiate the classes. Title the plot 'Equally Distributed Classes' to highlight the balanced nature of the dataset, with a font size set for clarity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0abc31ee-a78e-43af-822f-f06772d00c1c",
        "_uuid": "88477bac6687f110e9d64ec22837c250d85d2a2b",
        "id": "06gC8IMbnaJk"
      },
      "source": [
        "<h3> Correlation Matrices </h3>\n",
        "Correlation matrices are the essence of understanding our data. We want to know if there are features that influence heavily in whether a specific transaction is a fraud. However, it is important that we use the correct dataframe (subsample)  in order for us to see which features have a high positive or negative correlation with regards to fraud transactions.\n",
        "\n",
        "### Summary and Explanation:\n",
        "<ul>\n",
        "<li><b>Negative Correlations: </b>V17, V14, V12 and V10 are negatively correlated. Notice how the lower these values are, the more likely the end result will be a fraud transaction.  </li>\n",
        "<li> <b> Positive Correlations: </b> V2, V4, V11, and V19 are positively correlated. Notice how the higher these values are, the more likely the end result will be a fraud transaction. </li>\n",
        "<li> <b>BoxPlots: </b>  We will use boxplots to have a better understanding of the distribution of these features in fradulent and non fradulent transactions. </li>\n",
        "</ul>\n",
        "\n",
        "\n",
        "**Note: ** We have to make sure we use the subsample in our correlation matrix or else our correlation matrix will be affected by the high imbalance between our classes. This occurs due to the high class imbalance in the original dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9f353623-9435-4bb2-b854-b4a201ec7dd9",
        "_kg_hide-input": true,
        "_uuid": "e2f417c5d7c633a1e3cdfaa78acd6bd77a38400e",
        "id": "aXJ44YJpnaJl",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Make sure we use the subsample in our correlation\n",
        "\n",
        "f, (ax1, ax2) = plt.subplots(2, 1, figsize=(24,20))\n",
        "\n",
        "# Entire DataFrame\n",
        "#TODO : Calculate the correlation matrix corr of the dataframe df and then use seaborn's heatmap function to visualize this matrix, applying a coolwarm_r color map.\n",
        "\n",
        "\n",
        "#TODO : Calculate the correlation matrix sub_sample_corr for the dataframe new_df, which presumably contains a subsampled dataset aimed at addressing class imbalance.\n",
        "#TODO : Visualize this correlation matrix using seaborn's heatmap function, applying a coolwarm_r color map for visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2f02c21f-daa3-4251-a8e9-acad09a5ce0f",
        "_kg_hide-input": true,
        "_uuid": "318d0e7e0443f99139be21c00a7abc663be26385",
        "id": "iT17owG_naJl",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "f, axes = plt.subplots(ncols=4, figsize=(20,4))\n",
        "\n",
        "# Negative Correlations with our Class (The lower our feature value the more likely it will be a fraud transaction)\n",
        "sns.boxplot(x=\"Class\", y=\"V17\", data=new_df, palette=colors, ax=axes[0])\n",
        "axes[0].set_title('V17 vs Class Negative Correlation')\n",
        "\n",
        "sns.boxplot(x=\"Class\", y=\"V14\", data=new_df, palette=colors, ax=axes[1])\n",
        "axes[1].set_title('V14 vs Class Negative Correlation')\n",
        "\n",
        "\n",
        "sns.boxplot(x=\"Class\", y=\"V12\", data=new_df, palette=colors, ax=axes[2])\n",
        "axes[2].set_title('V12 vs Class Negative Correlation')\n",
        "\n",
        "\n",
        "sns.boxplot(x=\"Class\", y=\"V10\", data=new_df, palette=colors, ax=axes[3])\n",
        "axes[3].set_title('V10 vs Class Negative Correlation')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b457b10e-c17c-4cb2-9719-6d4128377c9f",
        "_kg_hide-input": true,
        "_uuid": "7bfc46c028f8602ee949de83629082633aa47b2c",
        "id": "GJdcZNmJnaJl",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "f, axes = plt.subplots(ncols=4, figsize=(20,4))\n",
        "\n",
        "# Positive correlations (The higher the feature the probability increases that it will be a fraud transaction)\n",
        "sns.boxplot(x=\"Class\", y=\"V11\", data=new_df, palette=colors, ax=axes[0])\n",
        "axes[0].set_title('V11 vs Class Positive Correlation')\n",
        "\n",
        "sns.boxplot(x=\"Class\", y=\"V4\", data=new_df, palette=colors, ax=axes[1])\n",
        "axes[1].set_title('V4 vs Class Positive Correlation')\n",
        "\n",
        "\n",
        "sns.boxplot(x=\"Class\", y=\"V2\", data=new_df, palette=colors, ax=axes[2])\n",
        "axes[2].set_title('V2 vs Class Positive Correlation')\n",
        "\n",
        "\n",
        "sns.boxplot(x=\"Class\", y=\"V19\", data=new_df, palette=colors, ax=axes[3])\n",
        "axes[3].set_title('V19 vs Class Positive Correlation')\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
