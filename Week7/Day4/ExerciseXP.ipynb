{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŒŸ Exercise 1: Calculating Required Sample Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are planning an A/B test to evaluate the impact of a new email subject line on the open rate. Based on past data, you expect a small effect size of 0.3 (an increase from 20% to 23% in the open rate). You aim for an 80% chance (power = 0.8) of detecting this effect if it exists, with a 5% significance level (Î± = 0.05).\n",
    "\n",
    "    Calculate the required sample size per group using Pythonâ€™s statsmodels library.\n",
    "    What sample size is needed for each group to ensure your test is properly powered?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size required per group: 2941\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.power import NormalIndPower\n",
    "from statsmodels.stats.proportion import proportion_effectsize\n",
    "\n",
    "# Setting the parameters\n",
    "alpha = 0.05  # Level of significance\n",
    "power = 0.8   # Power of the test\n",
    "p1 = 0.20     # Open rate actual (control group)\n",
    "p2 = 0.23     # Open rate expected (test group)\n",
    "\n",
    "# Calculate the Cohen's h effect size\n",
    "effect_size = proportion_effectsize(p1, p2)\n",
    "\n",
    "# Calculate the required sample size per group\n",
    "sample_size = NormalIndPower().solve_power(effect_size, power=power, alpha=alpha, ratio=1, alternative=\"two-sided\")\n",
    "\n",
    "# Display the result\n",
    "print(f\"Sample size required per group: {round(sample_size)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŒŸ Exercise 2: Understanding the Relationship Between Effect Size and Sample Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same A/B test setup as in Exercise 1, you want to explore how changing the expected effect size impacts the required sample size.\n",
    "\n",
    "    Calculate the required sample size for the following effect sizes: 0.2, 0.4, and 0.5, keeping the significance level and power the same.\n",
    "    How does the sample size change as the effect size increases? Explain why this happens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size needed for an effect of 0.2: 392\n",
      "Sample size needed for an effect of 0.4: 98\n",
      "Sample size needed for an effect of 0.5: 63\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.power import TTestIndPower\n",
    "\n",
    "\n",
    "# List of effect sizes (Cohen's h)\n",
    "effect_sizes = [0.2, 0.4, 0.5]\n",
    "\n",
    "\n",
    "sample_sizes = {}\n",
    "for effect_size in effect_sizes:\n",
    "    sample_size = NormalIndPower().solve_power(effect_size, power=power, alpha=alpha, ratio=1, alternative=\"two-sided\")\n",
    "    sample_sizes[effect_size] = round(sample_size)\n",
    "\n",
    "\n",
    "for effect_size, size in sample_sizes.items():\n",
    "    print(f\"Sample size needed for an effect of {effect_size}: {size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŒŸ Exercise 3: Exploring the Impact of Statistical Power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you are conducting an A/B test where you expect a small effect size of 0.2. You initially plan for a power of 0.8 but wonder how increasing or decreasing the desired power level impacts the required sample size.\n",
    "\n",
    "    Calculate the required sample size for power levels of 0.7, 0.8, and 0.9, keeping the effect size at 0.2 and significance level at 0.05.\n",
    "    Question: How does the required sample size change with different levels of statistical power? Why is this understanding important when designing A/B tests?\n",
    "'''\n",
    "As the power level increases, the required sample size also increases'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Power: 0.7, Required Sample Size per Group: 309\n",
      "Power: 0.8, Required Sample Size per Group: 393\n",
      "Power: 0.9, Required Sample Size per Group: 526\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.power import TTestIndPower\n",
    "\n",
    "# Define parameters\n",
    "effect_size = 0.2  # Small effect size\n",
    "alpha = 0.05       # Significance level\n",
    "power_levels = [0.7, 0.8, 0.9]  # Different power levels to evaluate\n",
    "\n",
    "\n",
    "analysis = TTestIndPower()\n",
    "\n",
    "sample_sizes = {}\n",
    "for power in power_levels:\n",
    "    sample_size = analysis.solve_power(effect_size=effect_size, alpha=alpha, power=power, alternative='two-sided')\n",
    "    sample_sizes[power] = int(sample_size)\n",
    "\n",
    "\n",
    "for power, sample_size in sample_sizes.items():\n",
    "    print(f\"Power: {power}, Required Sample Size per Group: {sample_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŒŸ Exercise 4: Implementing Sequential Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll stop the test early if one version clearly outperforms the other with a p-value below 0.05, but only if I've collected enough data to be confident in the results.\n",
    "\n",
    "Before making any decisions,I need to reach a minimum sample size per group, based on a power analysis, to ensure the results are reliable.\n",
    "\n",
    "It's not just about statistical significanceâ€”I also want to see a meaningful improvement in conversion rate (at least 5% increase) to justify making a change.\n",
    "\n",
    "Finally, if I do see a significant difference, Iâ€™ll wait at least two weeks to make sure the trend holds before making a final call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŒŸ Exercise 5: Applying Bayesian A/B Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Describe how you would set up your prior belief.**\n",
    "At first, I'll think the new feature has a 50% chance of making the app better, like flipping a coin. So,I can start with an even guess â€” it could be better, or it might not be.\n",
    "\n",
    "**After collecting data, how does the updated belief (posterior distribution) influence your decision?**\n",
    "After I get some results from testing, I update my guess based on the new information. If the test shows a 65% chance that the feature is better, I feel a little more confident that it helps, but i'm still not 100% sure.\n",
    "\n",
    "If I'm comfortable with taking chances, I might start using it. But if I want to be super sure, I might wait and get more results to feel safer abou tmy decision.\n",
    "\n",
    "**What would you do if the posterior probability was only 55%?**\n",
    "Itâ€™s better, but just a tiny bit : check more results, look for bigger changes,try againa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŒŸ Exercise 6: Implementing Adaptive Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain how you would adjust the traffic allocation after the first week**\n",
    "Since Layout C shows higher engagement after the first week, I can shift more traffic to Layout C to gather more data for it. For example, I might adjust the traffic allocation to 50% for Layout C and 25% each for Layout A and B. This allows me to focus more on the layout that's performing better, while still keeping some traffic for the other layouts to confirm the results.\n",
    "\n",
    "**Describe how you would continue to adapt the experiment in the following weeks.**\n",
    "I will continue monitoring the engagement for all three layouts. If Layout Câ€™s lead remains steady, I can keep increasing its traffic share.\n",
    "As I gain more confidence in Layout C's performance, I might allocate even more traffic to it while reducing the share for the other layouts, ensuring a larger sample size for the winning layout.If Layout A or B starts showing better results, I can adjust the traffic again or consider ending the experiment if Layout C continues to perform significantly better.\n",
    "\n",
    "**What challenges might you face with adaptive experimentation, and how would you address them?**\n",
    " Changing the traffic distribution too early can introduce bias, especially if I stop the test too soon. To mitigate this, I would use statistical methods like Bayesian analysis to avoid making premature decisions without sufficient data.\n",
    "\n",
    " If I focus too much on the early data, I might become overly confident in Layout C, even if the early results are just a fluke. To prevent this, I will run the experiment for an appropriate duration, ensuring I account for any randomness and validate the results.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
