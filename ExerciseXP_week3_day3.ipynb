{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8oWEjzKMPBl6xcY0ppeER",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TyamanovaMargo/DI-Bootcamp/blob/main/ExerciseXP_week3_day3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "fHciqNOmTgJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŒŸ Exercise 1: Duplicate Detection and Removal"
      ],
      "metadata": {
        "id": "f7elJHniTXjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Titanic dataset\n",
        "titanic_data = pd.read_csv('train.csv')\n",
        "\n",
        "# Explore the dataset\n",
        "print(titanic_data.head())\n",
        "# print(titanic_data.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1qWqBLrUD3H",
        "outputId": "e0b3255a-01ae-4794-cb0b-ec374fa91d5a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   PassengerId  Survived  Pclass  \\\n",
            "0            1         0       3   \n",
            "1            2         1       1   \n",
            "2            3         1       3   \n",
            "3            4         1       1   \n",
            "4            5         0       3   \n",
            "\n",
            "                                                Name     Sex   Age  SibSp  \\\n",
            "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
            "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
            "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
            "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
            "4                           Allen, Mr. William Henry    male  35.0      0   \n",
            "\n",
            "   Parch            Ticket     Fare Cabin Embarked  \n",
            "0      0         A/5 21171   7.2500   NaN        S  \n",
            "1      0          PC 17599  71.2833   C85        C  \n",
            "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
            "3      0            113803  53.1000  C123        S  \n",
            "4      0            373450   8.0500   NaN        S  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_duplicates = titanic_data.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {num_duplicates}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWXnyaKNTeRq",
        "outputId": "6b490372-c87b-4b96-ceb0-b21b1a710b5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of duplicate rows: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we don't have dublicates in train.csv"
      ],
      "metadata": {
        "id": "_BF7JRIRU31i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "so add 5 dublicates"
      ],
      "metadata": {
        "id": "X9n0tpHLWGa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_data = pd.concat([titanic_data, titanic_data.head()], ignore_index=True)\n",
        "\n",
        "\n",
        "print(f\"Number of duplicate rows: {titanic_data.duplicated().sum()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GcAgRdoWJIA",
        "outputId": "f8f735db-dfd1-4512-deda-e057e9b10c75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of duplicate rows: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_duplicates = titanic_data.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {num_duplicates}\")\n",
        "\n",
        "num_rows_before = titanic_data.shape[0]\n",
        "print(f\"Number of rows before duplicate removal: {num_rows_before}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CO7tRHZWTbZ",
        "outputId": "2b526c9e-a2df-4927-f25c-225220bf3dc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of duplicate rows: 5\n",
            "Number of rows before duplicate removal: 896\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "titanic_data = titanic_data.drop_duplicates()\n",
        "\n",
        "num_rows_after = titanic_data.shape[0]\n",
        "print(f\"Number of rows after duplicate removal: {num_rows_after}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I29nSXM7etA0",
        "outputId": "7fdb2b4b-ba26-4af8-b22f-814d66a0b0eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows after duplicate removal: 891\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŒŸ Exercise 2: Handling Missing Values"
      ],
      "metadata": {
        "id": "GormZYTUf-Gp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*Identify columns in the Titanic dataset with missing values.\n",
        "Explore different strategies for handling missing data, such as removal, imputation, and filling with a constant value.\n",
        "Apply each strategy to different columns based on the nature of the data.*\n",
        "\n",
        "Hint: Review methods like dropna(), fillna(), and SimpleImputer from scikit-learn."
      ],
      "metadata": {
        "id": "urgsFqrmCz3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer"
      ],
      "metadata": {
        "id": "TY0rRqU-G0cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_data = pd.read_csv('train.csv')\n",
        "\n",
        "# find missing values in Titanic\n",
        "missing_data = titanic_data.isnull()\n",
        "print(missing_data.head())# False (no missing value) or True (missing value).\n",
        "\n",
        "# Count missing values in each column\n",
        "missing_counts = titanic_data.isnull().sum()\n",
        "print(missing_counts)"
      ],
      "metadata": {
        "id": "mBR36ssSf_K3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8ddfc6b-7d9e-4945-bcb9-ded3c0f24cdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   PassengerId  Survived  Pclass   Name    Sex    Age  SibSp  Parch  Ticket  \\\n",
            "0        False     False   False  False  False  False  False  False   False   \n",
            "1        False     False   False  False  False  False  False  False   False   \n",
            "2        False     False   False  False  False  False  False  False   False   \n",
            "3        False     False   False  False  False  False  False  False   False   \n",
            "4        False     False   False  False  False  False  False  False   False   \n",
            "\n",
            "    Fare  Cabin  Embarked  \n",
            "0  False   True     False  \n",
            "1  False  False     False  \n",
            "2  False   True     False  \n",
            "3  False  False     False  \n",
            "4  False   True     False  \n",
            "PassengerId      0\n",
            "Survived         0\n",
            "Pclass           0\n",
            "Name             0\n",
            "Sex              0\n",
            "Age            177\n",
            "SibSp            0\n",
            "Parch            0\n",
            "Ticket           0\n",
            "Fare             0\n",
            "Cabin          687\n",
            "Embarked         2\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> method dropna()\n",
        "\n"
      ],
      "metadata": {
        "id": "Lx4MFwEjI-FC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove rows with missing values\n",
        "titanic_data.dropna()\n",
        "\n",
        "# Remove columns with missing values\n",
        "titanic_data.dropna(axis=1)\n",
        "\n",
        "# Count missing values in each column\n",
        "missing_counts = titanic_data.isnull().sum()\n",
        "print(missing_counts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPqB6WoqCunm",
        "outputId": "c5baec28-9fa5-4def-992e-5085f5660b33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PassengerId    0\n",
            "Survived       0\n",
            "Pclass         0\n",
            "Name           0\n",
            "Sex            0\n",
            "Age            0\n",
            "SibSp          0\n",
            "Parch          0\n",
            "Ticket         0\n",
            "Fare           0\n",
            "Cabin          0\n",
            "Embarked       0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> method filna()\n",
        "\n"
      ],
      "metadata": {
        "id": "HexiTVKVIysp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing 'Embarked' values with the most common category\n",
        "most_common_embarked = titanic_data['Embarked'].mode()[0]\n",
        "titanic_data['Embarked'].fillna(\"replaced\", inplace=True)\n"
      ],
      "metadata": {
        "id": "5dnEApJIHY1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "most_common_embarked = titanic_data['Age'].mode()[0]\n",
        "titanic_data['Age'].fillna(111, inplace=True)"
      ],
      "metadata": {
        "id": "jG-aWm40IQQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "most_common_embarked = titanic_data['Cabin'].mode()[0]\n",
        "titanic_data['Cabin'].fillna(\"not found\", inplace=True)"
      ],
      "metadata": {
        "id": "KkM9N-qRIgS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_counts = titanic_data.isnull().sum()\n",
        "print(missing_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0T6Ruk4VILTD",
        "outputId": "a04ffb84-94d8-473a-e894-5056da51b1e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PassengerId    0\n",
            "Survived       0\n",
            "Pclass         0\n",
            "Name           0\n",
            "Sex            0\n",
            "Age            0\n",
            "SibSp          0\n",
            "Parch          0\n",
            "Ticket         0\n",
            "Fare           0\n",
            "Cabin          0\n",
            "Embarked       0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imputer = SimpleImputer(strategy='median')\n",
        "titanic_data['Age'] = imputer.fit_transform(titanic_data[['Age']])"
      ],
      "metadata": {
        "id": "7aYrcw7qKeW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(titanic_data.sum())"
      ],
      "metadata": {
        "id": "4IqPxuLOKlz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(titanic_data.dtypes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3_jl2rCLAyi",
        "outputId": "45a0aa18-cc7f-41f2-8d98-ed982fd44859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PassengerId      int64\n",
            "Survived         int64\n",
            "Pclass           int64\n",
            "Name            object\n",
            "Sex             object\n",
            "Age            float64\n",
            "SibSp            int64\n",
            "Parch            int64\n",
            "Ticket          object\n",
            "Fare           float64\n",
            "Cabin           object\n",
            "Embarked        object\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(titanic_data.select_dtypes(include=['int']).sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RstaL7iPLLJT",
        "outputId": "4c31998c-fac9-44dc-b978-352a225b69f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PassengerId    397386\n",
            "Survived          342\n",
            "Pclass           2057\n",
            "SibSp             466\n",
            "Parch             340\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ðŸŒŸ Exercise 3: Feature Engineering"
      ],
      "metadata": {
        "id": "-SC8cJZfC4zQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Create new features, such as Family Size from SibSp and Parch, and Title extracted from the Name column.\n",
        "Convert categorical variables into numerical form using techniques like one-hot encoding or label encoding.\n",
        "Normalize or standardize numerical features if required.*\n",
        "\n",
        "Hint: Utilize Pandas for data manipulation and scikit-learnâ€™s preprocessing module for encoding."
      ],
      "metadata": {
        "id": "isp5qWsCC8zq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler, MinMaxScaler"
      ],
      "metadata": {
        "id": "p6kIbAJaHFjr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Titanic dataset\n",
        "titanic_data = pd.read_csv('train.csv')\n",
        "print(titanic_data.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpylZnQShKRy",
        "outputId": "90705f5b-3e94-4234-b1ac-38c18f71bc97"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
            "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "titanic_data = pd.read_csv('train.csv')\n",
        "\n",
        "\n",
        "titanic_data['FamilySize'] = titanic_data['SibSp'] + titanic_data['Parch'] + 1\n",
        "\n",
        "titanic_data['Title'] = titanic_data['Name'].str.extract(r' ([A-Za-z]+)\\.')\n",
        "\n",
        "\n",
        "titanic_data = pd.get_dummies(titanic_data, columns=['Embarked', 'Title'], drop_first=True)\n",
        "\n",
        "le = LabelEncoder()\n",
        "titanic_data['Sex'] = le.fit_transform(titanic_data['Sex'])\n",
        "\n",
        "print(titanic_data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2g_riMFe0x6G",
        "outputId": "b8fbefc8-c02e-4cb5-bb50-52f41e2226d2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   PassengerId  Survived  Pclass  \\\n",
            "0            1         0       3   \n",
            "1            2         1       1   \n",
            "2            3         1       3   \n",
            "3            4         1       1   \n",
            "4            5         0       3   \n",
            "\n",
            "                                                Name  Sex   Age  SibSp  Parch  \\\n",
            "0                            Braund, Mr. Owen Harris    1  22.0      1      0   \n",
            "1  Cumings, Mrs. John Bradley (Florence Briggs Th...    0  38.0      1      0   \n",
            "2                             Heikkinen, Miss. Laina    0  26.0      0      0   \n",
            "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)    0  35.0      1      0   \n",
            "4                           Allen, Mr. William Henry    1  35.0      0      0   \n",
            "\n",
            "             Ticket     Fare  ... Title_Major  Title_Master  Title_Miss  \\\n",
            "0         A/5 21171   7.2500  ...       False         False       False   \n",
            "1          PC 17599  71.2833  ...       False         False       False   \n",
            "2  STON/O2. 3101282   7.9250  ...       False         False        True   \n",
            "3            113803  53.1000  ...       False         False       False   \n",
            "4            373450   8.0500  ...       False         False       False   \n",
            "\n",
            "   Title_Mlle  Title_Mme  Title_Mr  Title_Mrs  Title_Ms  Title_Rev  Title_Sir  \n",
            "0       False      False      True      False     False      False      False  \n",
            "1       False      False     False       True     False      False      False  \n",
            "2       False      False     False      False     False      False      False  \n",
            "3       False      False     False       True     False      False      False  \n",
            "4       False      False      True      False     False      False      False  \n",
            "\n",
            "[5 rows x 30 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ðŸŒŸ Exercise 4: Outlier Detection and Handling\n"
      ],
      "metadata": {
        "id": "Jepc5ykzDWIO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*Use statistical methods to detect outliers in columns like Fare and Age.\n",
        "Decide on a strategy to handle the identified outliers, such as capping, transformation, or removal.\n",
        "Implement the chosen strategy and assess its impact on the dataset.*\n",
        "\n",
        "Hint: Explore methods like IQR (Interquartile Range) and Z-score for outlier detection."
      ],
      "metadata": {
        "id": "UuH_n2vlDa6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Q1 = titanic_data['Fare'].quantile(0.25)\n",
        "Q3 = titanic_data['Fare'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "\n",
        "titanic_data['Fare'] = np.where(titanic_data['Fare'] > upper_bound, upper_bound, titanic_data['Fare'])\n",
        "titanic_data['Fare'] = np.where(titanic_data['Fare'] < lower_bound, lower_bound, titanic_data['Fare'])\n",
        "\n",
        "\n",
        "from scipy.stats import zscore\n",
        "titanic_data['Age_Zscore'] = zscore(titanic_data['Age'])\n",
        "\n",
        "\n",
        "titanic_data = titanic_data[(titanic_data['Age_Zscore'] < 3) & (titanic_data['Age_Zscore'] > -3)]\n",
        "\n",
        "print(titanic_data[['Fare', 'Age']].describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KYJcW52050-",
        "outputId": "2b65df2a-2b11-4780-eb13-8b211eeb44fb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Fare  Age\n",
            "count   0.0  0.0\n",
            "mean    NaN  NaN\n",
            "std     NaN  NaN\n",
            "min     NaN  NaN\n",
            "25%     NaN  NaN\n",
            "50%     NaN  NaN\n",
            "75%     NaN  NaN\n",
            "max     NaN  NaN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ðŸŒŸ Exercise 5: Data Standardization and Normalization"
      ],
      "metadata": {
        "id": "uQVwKIEWDlEP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Assess the scale and distribution of numerical columns in the dataset.\n",
        "Apply standardization to features with a wide range of values.\n",
        "Normalize data that requires a bounded range, like [0, 1].\n",
        "\n",
        "Hint: Consider using StandardScaler and MinMaxScaler from scikit-learnâ€™s preprocessing module."
      ],
      "metadata": {
        "id": "nJKfoFcsDoUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "scaler = StandardScaler()\n",
        "titanic_data[['Age', 'Fare']] = scaler.fit_transform(titanic_data[['Age', 'Fare']])\n",
        "\n",
        "\n",
        "minmax_scaler = MinMaxScaler()\n",
        "titanic_data[['FamilySize']] = minmax_scaler.fit_transform(titanic_data[['FamilySize']])\n",
        "\n",
        "print(titanic_data[['Age', 'Fare', 'FamilySize']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "rff0kx7h1CK7",
        "outputId": "3370e0c5-9923-4b2c-bd05-ccb87ae7692b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Found array with 0 sample(s) (shape=(0, 2)) while a minimum of 1 is required by StandardScaler.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-7e2a20c938c5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtitanic_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Age'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Fare'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitanic_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Age'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Fare'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mminmax_scaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    892\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_fit_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefer_skip_nested_validation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \"\"\"\n\u001b[1;32m    929\u001b[0m         \u001b[0mfirst_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"n_samples_seen_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m         X = validate_data(\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2942\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2944\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2945\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2946\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1131\u001b[0m                 \u001b[0;34m\"Found array with %d sample(s) (shape=%s) while a\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m                 \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 2)) while a minimum of 1 is required by StandardScaler."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ðŸŒŸ Exercise 6: Feature Encoding"
      ],
      "metadata": {
        "id": "qjwN_9aiDsxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*Identify categorical columns in the Titanic dataset, such as Sex and Embarked.\n",
        "Use one-hot encoding for nominal variables and label encoding for ordinal variables.\n",
        "Integrate the encoded features back into the main dataset.*\n",
        "\n",
        "Hint: Utilize pandas.get_dummies() for one-hot encoding and LabelEncoder from scikit-learn for label encoding."
      ],
      "metadata": {
        "id": "EMl8eG3oDvIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "titanic_data = pd.read_csv(\"train.csv\")\n",
        "\n",
        "\n",
        "categorical_columns = ['Sex', 'Embarked']\n",
        "\n",
        "\n",
        "titanic_data = pd.get_dummies(titanic_data, columns=['Embarked'], prefix='Embarked')\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "titanic_data['Sex'] = label_encoder.fit_transform(titanic_data['Sex'])\n",
        "\n",
        "print(titanic_data[['Sex', 'Embarked_C', 'Embarked_Q', 'Embarked_S']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkS9s-JE16X6",
        "outputId": "88bdcfbf-0ddb-45d6-8cc6-18b1e415ffa4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Sex  Embarked_C  Embarked_Q  Embarked_S\n",
            "0    1       False       False        True\n",
            "1    0        True       False       False\n",
            "2    0       False       False        True\n",
            "3    0       False       False        True\n",
            "4    1       False       False        True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ðŸŒŸ Exercise 7: Data Transformation for Age Feature"
      ],
      "metadata": {
        "id": "m3jhmdCZD4oM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*Create age groups (bins) from the Age column to categorize passengers into different age categories.\n",
        "Apply one-hot encoding to the age groups to convert them into binary features.*\n",
        "\n",
        "Hint: Use pd.cut() for binning the Age column and pd.get_dummies() for one-hot encoding."
      ],
      "metadata": {
        "id": "nkFop6n5D5iI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "age = [0, 14, 18, 25, 60, 90]\n",
        "labels = ['Child', 'Teenager', 'YoungAdult', 'Adult', 'Senior']\n",
        "titanic_data['AgeGroup'] = pd.cut(titanic_data['Age'], bins=age, labels=labels)\n",
        "\n",
        "\n",
        "titanic_data = pd.get_dummies(titanic_data, columns=['AgeGroup'], prefix='Age')\n",
        "\n",
        "\n",
        "print(titanic_data[['Age', 'Age_Child', 'Age_Teenager', 'Age_YoungAdult', 'Age_Adult', 'Age_Senior']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gppt81S1-lx",
        "outputId": "47beb893-c959-405a-8607-803d975abb96"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Age  Age_Child  Age_Teenager  Age_YoungAdult  Age_Adult  Age_Senior\n",
            "0  22.0      False         False            True      False       False\n",
            "1  38.0      False         False           False       True       False\n",
            "2  26.0      False         False           False       True       False\n",
            "3  35.0      False         False           False       True       False\n",
            "4  35.0      False         False           False       True       False\n"
          ]
        }
      ]
    }
  ]
}