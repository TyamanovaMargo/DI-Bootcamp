{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM13/OzrniVgiUgc6UXVs7K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TyamanovaMargo/DI-Bootcamp/blob/main/exerciseXP_Gold.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 1 : Analyzing Confusion Matrix"
      ],
      "metadata": {
        "id": "80z5hPlvC4me"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Instructions**\n",
        "\n",
        "Imagine you have a dataset for a binary classification problem, such as email spam detection, where emails are classified as either ‘Spam’ or ‘Not Spam’. You are provided with the confusion matrix results of a classifier.\n",
        "- Define in your own words what True Positives, True Negatives, False Positives, and False Negatives mean in the context of this email spam detection problem.\n",
        "- Given a confusion matrix with specific values for TP, TN, FP, FN, calculate the Accuracy, Precision, Recall, and F1-Score.\n",
        "- Discuss how the classifier’s performance would change with a higher number of False Positives compared to False Negatives, and vice versa."
      ],
      "metadata": {
        "id": "9WbeC1NQC-OL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definitions in the context of email spam detection\n",
        "true_positives_definition = \"Emails correctly classified as Spam\"\n",
        "true_negatives_definition = \"Emails correctly classified as Not Spam\"\n",
        "false_positives_definition = \"Non-Spam emails incorrectly classified as Spam\"\n",
        "false_negatives_definition = \"Spam emails incorrectly classified as Not Spam\"\n",
        "\n",
        "# Example Confusion Matrix Values\n",
        "TP, TN, FP, FN = 80, 50, 10, 5  # Example values\n",
        "\n",
        "# Calculating Evaluation Metrics\n",
        "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "precision = TP / (TP + FP)\n",
        "recall = TP / (TP + FN)\n",
        "f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-Score: {f1_score}\")\n",
        "\n",
        "# Discussion on classifier's performance change with varying FP and FN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADIJavMyDJFr",
        "outputId": "a18e240e-095b-45a0-d5bb-f834028c94a0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.896551724137931, Precision: 0.8888888888888888, Recall: 0.9411764705882353, F1-Score: 0.9142857142857143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 2: Evaluating Trade-offs in Metrics"
      ],
      "metadata": {
        "id": "EYtRhcSjDO_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importance of Recall over Precision in Medical Diagnosis\n",
        "recall_importance_medical = \"\"\"\n",
        "High recall is crucial in medical diagnosis as it reduces the number of False Negatives, which are critical in this context. Missing a true case of the disease could be life-threatening.\n",
        "\"\"\"\n",
        "\n",
        "# Scenario where Precision is more important than Recall\n",
        "precision_important_scenario = \"\"\"\n",
        "In email spam detection, precision is more critical as False Positives (legitimate emails marked as spam) can be more problematic than False Negatives (spam emails not detected).\n",
        "\"\"\"\n",
        "\n",
        "# Consequences of focusing solely on Accuracy\n",
        "accuracy_consequences = \"\"\"\n",
        "Focusing only on accuracy in imbalanced datasets can be misleading as the model might simply predict the majority class most of the time, ignoring the minority class which is often of greater interest.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "oUHweGVsDR0d"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 3: Understanding Cross-Validation and Learning Curves"
      ],
      "metadata": {
        "id": "cz-oZ74jDW7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Difference between K-Fold and Stratified K-Fold Cross-Validation\n",
        "k_fold_vs_stratified = \"\"\"\n",
        "K-Fold divides the data into 'K' equal parts without considering the distribution of classes, whereas Stratified K-Fold divides data such that each fold maintains the same percentage of samples for each class.\n",
        "\"\"\"\n",
        "\n",
        "# Learning Curves Explanation\n",
        "learning_curves_explanation = \"\"\"\n",
        "Learning curves plot the model's performance on both the training set and validation set over varying levels of training data or complexity. They help in diagnosing problems like overfitting or underfitting.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "3xKet05WDXrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 4: Impact of Class Imbalance on Model Evaluation"
      ],
      "metadata": {
        "id": "o--xvKEmDgrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Misleading nature of Accuracy in Imbalanced Datasets\n",
        "accuracy_misleading = \"\"\"\n",
        "In datasets with a significant class imbalance, accuracy can be misleading as it can be high even if the model is only predicting the majority class correctly.\n",
        "\"\"\"\n",
        "\n",
        "# Strategies for Evaluating and Improving Model Performance in Imbalanced Datasets\n",
        "imbalance_strategies = \"\"\"\n",
        "Use metrics like Precision, Recall, and F1-Score. Consider using techniques like SMOTE for oversampling the minority class or adjusting class weights in the model.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "JUugaJF5DjwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 5: Role of Threshold Tuning in Classification Models"
      ],
      "metadata": {
        "id": "IIWwpbHBDmFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Impact of Threshold Adjustment\n",
        "threshold_impact = \"\"\"\n",
        "Increasing the threshold from 0.5 to 0.7 for a positive class can increase precision (fewer False Positives) but may decrease recall (more False Negatives).\n",
        "\"\"\"\n",
        "\n",
        "# ROC and AUC in Threshold Determination\n",
        "roc_auc_explanation = \"\"\"\n",
        "ROC curves plot the True Positive Rate against the False Positive Rate at various threshold settings. AUC provides an aggregate measure of performance across all possible thresholds. They help in finding the optimal balance between sensitivity and specificity.\n",
        "\"\"\"\n",
        "\n",
        "# Note: This script provides a theoretical understanding and framework for each exercise. Actual data and specific scenarios will require tailored analysis and application of these concepts."
      ],
      "metadata": {
        "id": "y4yNym80DoEE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}