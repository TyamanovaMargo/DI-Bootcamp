{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxUbN4trfIRKLx0XoiUFQE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TyamanovaMargo/DI-Bootcamp/blob/main/exerciseXP_Ninja.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 1 : Evaluation Metrics Implementation"
      ],
      "metadata": {
        "id": "TSHqh2aVEYKm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement Python functions to calculate Accuracy, Precision, Recall, and F1-Score from scratch"
      ],
      "metadata": {
        "id": "V30zDKn6EeP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "KCI-1eEOFnLT"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUr-arBED-qE",
        "outputId": "d2c13259-bf88-494f-c738-6acf7f310b93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
            "0    842302         M        17.99         10.38          122.80     1001.0   \n",
            "1    842517         M        20.57         17.77          132.90     1326.0   \n",
            "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
            "3  84348301         M        11.42         20.38           77.58      386.1   \n",
            "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
            "\n",
            "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
            "0          0.11840           0.27760          0.3001              0.14710   \n",
            "1          0.08474           0.07864          0.0869              0.07017   \n",
            "2          0.10960           0.15990          0.1974              0.12790   \n",
            "3          0.14250           0.28390          0.2414              0.10520   \n",
            "4          0.10030           0.13280          0.1980              0.10430   \n",
            "\n",
            "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
            "0  ...          17.33           184.60      2019.0            0.1622   \n",
            "1  ...          23.41           158.80      1956.0            0.1238   \n",
            "2  ...          25.53           152.50      1709.0            0.1444   \n",
            "3  ...          26.50            98.87       567.7            0.2098   \n",
            "4  ...          16.67           152.20      1575.0            0.1374   \n",
            "\n",
            "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
            "0             0.6656           0.7119                0.2654          0.4601   \n",
            "1             0.1866           0.2416                0.1860          0.2750   \n",
            "2             0.4245           0.4504                0.2430          0.3613   \n",
            "3             0.8663           0.6869                0.2575          0.6638   \n",
            "4             0.2050           0.4000                0.1625          0.2364   \n",
            "\n",
            "   fractal_dimension_worst  Unnamed: 32  \n",
            "0                  0.11890          NaN  \n",
            "1                  0.08902          NaN  \n",
            "2                  0.08758          NaN  \n",
            "3                  0.17300          NaN  \n",
            "4                  0.07678          NaN  \n",
            "\n",
            "[5 rows x 33 columns]\n",
            "Data shape after dropping 'Unnamed: 32': (569, 32)\n",
            "Data shape after dropping NaNs: (569, 32)\n",
            "Accuracy: 0.9590643274853801\n",
            "Precision: 0.9827586206896551\n",
            "Recall: 0.9047619047619048\n",
            "F1-Score: 0.9421487603305785\n",
            "\n",
            "    Accuracy gives the proportion of correct predictions among the total number of cases.\n",
            "    Precision tells us the proportion of positive identifications that were actually correct.\n",
            "    Recall gives the proportion of actual positives that were correctly identified.\n",
            "    F1-Score provides a balance between Precision and Recall.\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "# Load the CSV file into a DataFrame\n",
        "df = pd.read_csv('/content/data.csv')  # Replace with the correct path to your file\n",
        "\n",
        "# Display the first few rows to check the structure\n",
        "print(df.head())\n",
        "\n",
        "# Drop the 'Unnamed: 32' column (if it's unnecessary)\n",
        "df = df.drop(columns=['Unnamed: 32'])\n",
        "\n",
        "# Check the number of rows and columns after dropping the 'Unnamed: 32' column\n",
        "print(f\"Data shape after dropping 'Unnamed: 32': {df.shape}\")\n",
        "\n",
        "# Drop rows with missing values (if any)\n",
        "df = df.dropna()\n",
        "\n",
        "# Check the number of rows and columns after dropping NaN values\n",
        "print(f\"Data shape after dropping NaNs: {df.shape}\")\n",
        "\n",
        "# Define X (features) and y (target)\n",
        "X = df.drop('diagnosis', axis=1)  # Replace 'diagnosis' with the exact column name if necessary\n",
        "y = df['diagnosis']  # The 'diagnosis' column is the target\n",
        "\n",
        "# Convert the categorical target ('M' and 'B') to numerical values for modeling\n",
        "y = y.map({'M': 1, 'B': 0})\n",
        "\n",
        "# If the dataset is still empty after dropping NaNs, check the original data.\n",
        "if df.shape[0] == 0:\n",
        "    print(\"The dataset is empty after dropping NaN values. Check the original data.\")\n",
        "else:\n",
        "    # Split the dataset into training and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    # Train a Logistic Regression model\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Get confusion matrix components\n",
        "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "\n",
        "    # Function to calculate Accuracy\n",
        "    def calculate_accuracy(tp, tn, fp, fn):\n",
        "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "        return accuracy\n",
        "\n",
        "    # Function to calculate Precision\n",
        "    def calculate_precision(tp, fp):\n",
        "        precision = tp / (tp + fp)\n",
        "        return precision\n",
        "\n",
        "    # Function to calculate Recall\n",
        "    def calculate_recall(tp, fn):\n",
        "        recall = tp / (tp + fn)\n",
        "        return recall\n",
        "\n",
        "    # Function to calculate F1-Score\n",
        "    def calculate_f1_score(precision, recall):\n",
        "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "        return f1_score\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = calculate_accuracy(tp, tn, fp, fn)\n",
        "    precision = calculate_precision(tp, fp)\n",
        "    recall = calculate_recall(tp, fn)\n",
        "    f1 = calculate_f1_score(precision, recall)\n",
        "\n",
        "    # Print the evaluation metrics\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "    print(f\"F1-Score: {f1}\")\n",
        "\n",
        "    # Interpretation of the results\n",
        "    interpretation = \"\"\"\n",
        "    Accuracy gives the proportion of correct predictions among the total number of cases.\n",
        "    Precision tells us the proportion of positive identifications that were actually correct.\n",
        "    Recall gives the proportion of actual positives that were correctly identified.\n",
        "    F1-Score provides a balance between Precision and Recall.\n",
        "    \"\"\"\n",
        "    print(interpretation)\n",
        "\n"
      ]
    }
  ]
}